{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = d_model * expand\n",
    "        \n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner)\n",
    "        self.conv = nn.Conv1d(self.d_inner, self.d_inner, d_conv, padding=d_conv-1, groups=self.d_inner)\n",
    "        \n",
    "        self.x_proj = nn.Linear(self.d_inner, self.d_state)\n",
    "        self.dt_proj = nn.Linear(self.d_inner, self.d_state)\n",
    "        \n",
    "        self.A = nn.Parameter(torch.randn(self.d_state))\n",
    "        self.D = nn.Parameter(torch.randn(self.d_state, self.d_inner))\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, L, _ = x.shape\n",
    "        \n",
    "        x = self.in_proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv(x)[:, :, :L]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        x_ssm = self.x_proj(x)\n",
    "        dt = F.softplus(self.dt_proj(x))\n",
    "        \n",
    "        A = -torch.exp(self.A).view(1, 1, -1)\n",
    "        D = self.D\n",
    "        \n",
    "        dA = torch.exp(A * dt)\n",
    "        dB = (1 - dA) / A\n",
    "        \n",
    "        z = torch.zeros(B, self.d_state, device=x.device)\n",
    "        output = []\n",
    "        for i in range(L):\n",
    "            z = dA[:, i] * z + dB[:, i] * x_ssm[:, i]\n",
    "            output.append(z)\n",
    "        z = torch.stack(output, dim=1)\n",
    "        \n",
    "        y = torch.einsum('bls,si->bli', z, D)\n",
    "        \n",
    "        y = self.out_proj(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class MambaLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_state, d_conv, expand, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([MambaBlock(d_model, d_state, d_conv, expand) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = self.lm_head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def load_ptb_data(batch_size):\n",
    "    path = 'ptb.train.txt'\n",
    "    if not os.path.exists(path):\n",
    "        url = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\"\n",
    "        print(f\"Downloading dataset from {url}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Download completed.\")\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = f.read().split()\n",
    "    \n",
    "    vocab = ['<unk>', '<eos>'] + sorted(set(raw_data))\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "    \n",
    "    data = [word_to_idx[word] for word in raw_data]\n",
    "    \n",
    "    train_data = data[:int(0.8*len(data))]\n",
    "    val_data = data[int(0.8*len(data)):int(0.9*len(data))]\n",
    "    test_data = data[int(0.9*len(data)):]\n",
    "    \n",
    "    return train_data, val_data, test_data, len(vocab), idx_to_word\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.data[index:index+self.seq_length]),\n",
    "            torch.tensor(self.data[index+1:index+self.seq_length+1])\n",
    "        )\n",
    "\n",
    "def get_data_loaders(train_data, val_data, test_data, batch_size, seq_length):\n",
    "    train_dataset = TextDataset(train_data, seq_length)\n",
    "    val_dataset = TextDataset(val_data, seq_length)\n",
    "    test_dataset = TextDataset(test_data, seq_length)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                        desc=f\"Epoch {epoch}\", ncols=100)\n",
    "    \n",
    "    for batch, (data, target) in progress_bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / (batch + 1)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{avg_loss:.4f}',\n",
    "            'perplexity': f'{math.exp(avg_loss):.2f}'\n",
    "        })\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": batch,\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"train_perplexity\": math.exp(avg_loss),\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output.view(-1, output.size(-1)), target.view(-1)).item() * data.size(0)\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "def generate_text(model, idx_to_word, device, start_text=\"The\", max_length=50):\n",
    "    model.eval()\n",
    "    words = start_text.split()\n",
    "    word_to_idx = {v: k for k, v in idx_to_word.items()}\n",
    "    \n",
    "    input_seq = [word_to_idx.get(w, word_to_idx['<unk>']) for w in words]\n",
    "    input_seq = torch.tensor([input_seq]).to(device)\n",
    "    \n",
    "    generated_words = words.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output = model(input_seq)\n",
    "            next_word_idx = output[:, -1, :].argmax(dim=-1).item()\n",
    "            next_word = idx_to_word[next_word_idx]\n",
    "            generated_words.append(next_word)\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_word_idx]]).to(device)], dim=1)\n",
    "            \n",
    "            if next_word == '<eos>':\n",
    "                break\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "def train_mamba(batch_size=1024, seq_length=35, epochs=10, lr=0.001, d_model=256, d_state=16, d_conv=4, expand=2, num_layers=4):\n",
    "    wandb.init(project=\"mamba-lm\", config={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"d_model\": d_model,\n",
    "        \"d_state\": d_state,\n",
    "        \"d_conv\": d_conv,\n",
    "        \"expand\": expand,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        train_data, val_data, test_data, vocab_size, idx_to_word = load_ptb_data(batch_size)\n",
    "        train_loader, val_loader, test_loader = get_data_loaders(train_data, val_data, test_data, batch_size, seq_length)\n",
    "        print(f\"Data loaded. Vocabulary size: {vocab_size}\")\n",
    "\n",
    "        print(\"Initializing model...\")\n",
    "        model = MambaLM(vocab_size, d_model, d_state, d_conv, expand, num_layers).to(device)\n",
    "        wandb.watch(model, log_freq=100)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = train(model, train_loader, optimizer, criterion, device, epoch)\n",
    "            val_loss = evaluate(model, val_loader, criterion, device)\n",
    "            print(f'Epoch {epoch} - Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Perplexity: {math.exp(train_loss):.2f}, '\n",
    "                  f'Val Loss: {val_loss:.4f}, '\n",
    "                  f'Val Perplexity: {math.exp(val_loss):.2f}')\n",
    "            \n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_perplexity\": math.exp(train_loss),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_perplexity\": math.exp(val_loss),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_path = 'mamba_model_best.pt'\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'idx_to_word': idx_to_word\n",
    "                }, save_path)\n",
    "                # torch.save(model.state_dict(), 'mamba_model_best.pt')\n",
    "                wandb.save('mamba_model_best.pt')\n",
    "            \n",
    "            sample_text = generate_text(model, idx_to_word, device, start_text=\"The\", max_length=50)\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"generated_text\": wandb.Html(sample_text.replace('\\n', '<br>'))\n",
    "            })\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "        # model.load_state_dict(torch.load('mamba_model_best.pt'))\n",
    "        checkpoint = torch.load('mamba_model_best.pt')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        idx_to_word = checkpoint['idx_to_word']\n",
    "        print(\"Model and idx_to_word loaded successfully.\")\n",
    "        ###\n",
    "        test_loss = evaluate(model, test_loader, criterion, device)\n",
    "        test_perplexity = math.exp(test_loss)\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Perplexity: {test_perplexity:.2f}')\n",
    "        wandb.log({\"test_loss\": test_loss, \"test_perplexity\": test_perplexity})\n",
    "\n",
    "        return model, idx_to_word\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        wandb.finish()\n",
    "        return None, None\n",
    "\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrhnylmz14\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\serha\\OneDrive\\Masaüstü\\topoformer-ssm\\wandb\\run-20240709_193036-xurz3mb9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srhnylmz14/mamba-lm/runs/xurz3mb9' target=\"_blank\">vital-sea-4</a></strong> to <a href='https://wandb.ai/srhnylmz14/mamba-lm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srhnylmz14/mamba-lm' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srhnylmz14/mamba-lm/runs/xurz3mb9' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm/runs/xurz3mb9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Data loaded. Vocabulary size: 10001\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████| 694/694 [01:57<00:00,  5.92it/s, loss=6.3097, perplexity=549.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 6.3097, Train Perplexity: 549.87, Val Loss: 6.2041, Val Perplexity: 494.77\n",
      "An error occurred: [WinError 1314] A required privilege is not held by the client: 'c:\\\\Users\\\\serha\\\\OneDrive\\\\Masaüstü\\\\topoformer-ssm\\\\mamba_model_best.pt' -> 'c:\\\\Users\\\\serha\\\\OneDrive\\\\Masaüstü\\\\topoformer-ssm\\\\wandb\\\\run-20240709_193036-xurz3mb9\\\\files\\\\mamba_model_best.pt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\serha\\AppData\\Local\\Temp\\ipykernel_24724\\2367658541.py\", line 260, in train_mamba\n",
      "    wandb.save('mamba_model_best.pt')\n",
      "  File \"c:\\Users\\serha\\miniconda3\\envs\\ssm\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 400, in wrapper_fn\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\serha\\miniconda3\\envs\\ssm\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 390, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\serha\\miniconda3\\envs\\ssm\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1981, in save\n",
      "    return self._save(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\serha\\miniconda3\\envs\\ssm\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2045, in _save\n",
      "    target_path.symlink_to(source_path)\n",
      "  File \"c:\\Users\\serha\\miniconda3\\envs\\ssm\\Lib\\pathlib.py\", line 1198, in symlink_to\n",
      "    os.symlink(target, self, target_is_directory)\n",
      "OSError: [WinError 1314] A required privilege is not held by the client: 'c:\\\\Users\\\\serha\\\\OneDrive\\\\Masaüstü\\\\topoformer-ssm\\\\mamba_model_best.pt' -> 'c:\\\\Users\\\\serha\\\\OneDrive\\\\Masaüstü\\\\topoformer-ssm\\\\wandb\\\\run-20240709_193036-xurz3mb9\\\\files\\\\mamba_model_best.pt'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>train_perplexity</td><td>█▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_perplexity</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>600</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train_loss</td><td>6.30969</td></tr><tr><td>train_perplexity</td><td>549.87417</td></tr><tr><td>val_loss</td><td>6.2041</td></tr><tr><td>val_perplexity</td><td>494.77218</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vital-sea-4</strong> at: <a href='https://wandb.ai/srhnylmz14/mamba-lm/runs/xurz3mb9' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm/runs/xurz3mb9</a><br/> View project at: <a href='https://wandb.ai/srhnylmz14/mamba-lm' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240709_193036-xurz3mb9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.login()\n",
    "    model, idx_to_word = train_mamba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaLM(\n",
       "  (embedding): Embedding(50, 256)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x MambaBlock(\n",
       "      (in_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "      (x_proj): Linear(in_features=512, out_features=16, bias=True)\n",
       "      (dt_proj): Linear(in_features=512, out_features=16, bias=True)\n",
       "      (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=256, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '#',\n",
       " 3: '$',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: '*',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '/',\n",
       " 10: '0',\n",
       " 11: '1',\n",
       " 12: '2',\n",
       " 13: '3',\n",
       " 14: '4',\n",
       " 15: '5',\n",
       " 16: '6',\n",
       " 17: '7',\n",
       " 18: '8',\n",
       " 19: '9',\n",
       " 20: '<',\n",
       " 21: '>',\n",
       " 22: 'N',\n",
       " 23: '\\\\',\n",
       " 24: 'a',\n",
       " 25: 'b',\n",
       " 26: 'c',\n",
       " 27: 'd',\n",
       " 28: 'e',\n",
       " 29: 'f',\n",
       " 30: 'g',\n",
       " 31: 'h',\n",
       " 32: 'i',\n",
       " 33: 'j',\n",
       " 34: 'k',\n",
       " 35: 'l',\n",
       " 36: 'm',\n",
       " 37: 'n',\n",
       " 38: 'o',\n",
       " 39: 'p',\n",
       " 40: 'q',\n",
       " 41: 'r',\n",
       " 42: 's',\n",
       " 43: 't',\n",
       " 44: 'u',\n",
       " 45: 'v',\n",
       " 46: 'w',\n",
       " 47: 'x',\n",
       " 48: 'y',\n",
       " 49: 'z'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('mamba_model_best.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "idx_to_word = checkpoint['idx_to_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## To be tried after the training is done\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_to_char\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_text)\n",
      "Cell \u001b[1;32mIn[46], line 214\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(model, idx_to_char, start_text, max_length)\u001b[0m\n\u001b[0;32m    212\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    213\u001b[0m char_to_idx \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m idx_to_char\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 214\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43m[\u001b[49m\u001b[43mchar_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart_text\u001b[49m\u001b[43m]\u001b[49m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    216\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m start_text\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[46], line 214\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    212\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    213\u001b[0m char_to_idx \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m idx_to_char\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 214\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[43mchar_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m start_text]])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    216\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m start_text\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mKeyError\u001b[0m: 'T'"
     ]
    }
   ],
   "source": [
    "## To be tried after the training is done\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generated_text = generate_text(model, idx_to_word, device, start_text=\"The quick brown\", max_length=50)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
