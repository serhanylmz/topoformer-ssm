{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc189b1-3f2b-4d72-ac19-bd310f21f01d",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7556e-51d2-4ce2-ba05-b65a5b57120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mamba-ssm\n",
    "!pip install causal-conv1d>=1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbdf41-76d8-46bc-b275-895266b5c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy tqdm transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad4db9-1cb4-4a13-aea3-15d5affc89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfcb46-1229-4727-b6be-29777b17f2ac",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3c1100d-2d3e-49a4-86eb-911b4585482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def load_and_preprocess_data(max_length=128, stride=64):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_length=True,\n",
    "            stride=stride,\n",
    "        )\n",
    "        \n",
    "        input_batch = []\n",
    "        for length, input_ids in zip(tokenized_inputs[\"length\"], tokenized_inputs[\"input_ids\"]):\n",
    "            if length == max_length:\n",
    "                input_batch.append(input_ids)\n",
    "        \n",
    "        return {\"input_ids\": input_batch}\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    tokenized_dataset.set_format(type=\"torch\")\n",
    "    \n",
    "    return tokenized_dataset, tokenizer\n",
    "\n",
    "def create_dataloaders(dataset, batch_size=32):\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(dataset[\"validation\"], batch_size=batch_size)\n",
    "    test_dataloader = torch.utils.data.DataLoader(dataset[\"test\"], batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a49d1d1-315b-416b-9977-db7c633aa29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, tokenizer = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54a72079-cd46-46b8-92ae-289d779e2e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Train samples: 8624\n",
      "Validation samples: 922\n",
      "Test samples: 1017\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(dataset)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcc1a481-87b2-4dcd-a638-d72d1e5745ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([32, 128])\n",
      "Sample input:  Several terrestrial starlings, including those in the genus Sturnus, have adaptations of the skull and muscles that help with feeding by probing. This adaptation is most strongly developed in the common starling ( along with the spotless and white @-@ <unk> starlings ), where the <unk> muscles responsible for opening the jaw are enlarged and the skull is narrow, allowing the eye to be moved forward to peer down the length of the bill. This technique involves inserting the bill into the ground and opening it as a way of searching for hidden food items. Common starlings have the physical traits that enable them to use this feeding\n"
     ]
    }
   ],
   "source": [
    "# Check a sample batch\n",
    "for batch in train_dataloader:\n",
    "    print(\"Sample batch shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Sample input:\", tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d06f10-3317-4b9f-b505-47c6d2053770",
   "metadata": {},
   "source": [
    "### Mamba Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb2286ac-88a7-43ce-bfc5-7eb11fb93115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand):\n",
    "        super().__init__()\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self.mamba(x))\n",
    "\n",
    "class MambaLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layer, d_state, d_conv, expand):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            MambaBlock(d_model, d_state, d_conv, expand)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "def create_mamba_model(vocab_size, d_model=256, n_layer=4, d_state=16, d_conv=4, expand=2):\n",
    "    return MambaLM(vocab_size, d_model, n_layer, d_state, d_conv, expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52319f34-caa5-43b3-8157-903be36504b4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac2403f0-8c3d-4c7a-a045-5ced5c0a7f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1185dab0-8ee6-4ba8-819c-274020ffdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import math\n",
    "# from data_preparation import load_and_preprocess_data, create_dataloaders\n",
    "# from mamba_model import create_mamba_model\n",
    "\n",
    "def compute_metrics(logits, targets):\n",
    "    loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "    perplexity = math.exp(loss.item())\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    accuracy = (predictions == targets).float().mean().item()\n",
    "    return {\n",
    "        'loss': loss,  # Return the tensor, not the item\n",
    "        'loss_value': loss.item(),  # Add this for logging\n",
    "        'perplexity': perplexity,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, num_epochs, lr, device):\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    wandb.init(project=\"mamba-next-word-prediction\", config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"model_config\": model.config if hasattr(model, 'config') else None\n",
    "    })\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_metrics = {'train_loss': 0, 'train_perplexity': 0, 'train_accuracy': 0}\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            targets = input_ids.clone()\n",
    "            targets[:, :-1] = input_ids[:, 1:]\n",
    "            targets[:, -1] = input_ids[:, 0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            metrics = compute_metrics(outputs, targets)\n",
    "            loss = metrics['loss']  # This is now a tensor\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # Update epoch metrics\n",
    "            epoch_metrics['train_loss'] += metrics['loss_value']\n",
    "            epoch_metrics['train_perplexity'] += metrics['perplexity']\n",
    "            epoch_metrics['train_accuracy'] += metrics['accuracy']\n",
    "\n",
    "            if global_step % 50 == 0:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": metrics['loss_value'],\n",
    "                    \"train_perplexity\": metrics['perplexity'],\n",
    "                    \"train_accuracy\": metrics['accuracy']\n",
    "                }, step=global_step)\n",
    "\n",
    "            if global_step % 250 == 0:\n",
    "                val_metrics = evaluate(model, val_dataloader, device)\n",
    "                wandb.log(val_metrics, step=global_step)\n",
    "                model.train()  # Switch back to train mode after evaluation\n",
    "\n",
    "        # Log epoch-level metrics\n",
    "        epoch_metrics = {k: v / len(train_dataloader) for k, v in epoch_metrics.items()}\n",
    "        wandb.log(epoch_metrics, step=global_step)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_metrics['train_loss']:.4f}, \"\n",
    "              f\"Train Perplexity: {epoch_metrics['train_perplexity']:.4f}, \"\n",
    "              f\"Train Accuracy: {epoch_metrics['train_accuracy']:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    wandb.finish()\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_metrics = {'val_loss': 0, 'val_perplexity': 0, 'val_accuracy': 0}\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            targets = input_ids.clone()\n",
    "            targets[:, :-1] = input_ids[:, 1:]\n",
    "            targets[:, -1] = input_ids[:, 0]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            metrics = compute_metrics(outputs, targets)\n",
    "            total_metrics['val_loss'] += metrics['loss_value']\n",
    "            total_metrics['val_perplexity'] += metrics['perplexity']\n",
    "            total_metrics['val_accuracy'] += metrics['accuracy']\n",
    "\n",
    "    avg_metrics = {k: v / len(dataloader) for k, v in total_metrics.items()}\n",
    "    print(f\"Validation Loss: {avg_metrics['val_loss']:.4f}, \"\n",
    "          f\"Validation Perplexity: {avg_metrics['val_perplexity']:.4f}, \"\n",
    "          f\"Validation Accuracy: {avg_metrics['val_accuracy']:.4f}\")\n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a1179c2-c29c-43cb-9683-79277feecd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/topoformer-ssm/wandb/run-20240723_152056-lxcjjs8u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/lxcjjs8u' target=\"_blank\">eager-water-4</a></strong> to <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-next-word-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/lxcjjs8u' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/lxcjjs8u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/lxcjjs8u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fef49015120>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_epochs = 10\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "d_model=256\n",
    "n_layer=4\n",
    "d_state=16\n",
    "d_conv=4\n",
    "expand=2\n",
    "\n",
    "wandb.init(project=\"mamba-next-word-prediction\", config={\n",
    "    \"d_model\": d_model,\n",
    "    \"n_layer\": n_layer,\n",
    "    \"d_state\": d_state,\n",
    "    \"d_conv\": d_conv,\n",
    "    \"expand\": expand,\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38574099-6210-42e2-ac10-687cb53feeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, tokenizer = load_and_preprocess_data()\n",
    "train_dataloader, val_dataloader, _ = create_dataloaders(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e58e6230-a929-4588-ad3c-bfe8af4754e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Mamba model with parameters:\n",
      "d_model: 256\n",
      "n_layer: 4\n",
      "d_state: 16\n",
      "d_conv: 4\n",
      "expand: 2\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "model = create_mamba_model(\n",
    "    vocab_size,\n",
    "    d_model,\n",
    "    n_layer,\n",
    "    d_state,\n",
    "    d_conv,\n",
    "    expand\n",
    ")\n",
    "\n",
    "print(f\"Created Mamba model with parameters:\")\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"n_layer: {n_layer}\")\n",
    "print(f\"d_state: {d_state}\")\n",
    "print(f\"d_conv: {d_conv}\")\n",
    "print(f\"expand: {expand}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe4dec78-4c72-4ec6-ab93-2e2a1d54fe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:yfuizgb6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-yogurt-2</strong> at: <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/yfuizgb6' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/yfuizgb6</a><br/> View project at: <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-next-word-prediction</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240723_151213-yfuizgb6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:yfuizgb6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9aeee98c624438498da1b2ae3a62a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112978888887584, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/topoformer-ssm/wandb/run-20240723_151230-g9w95mbo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/g9w95mbo' target=\"_blank\">tough-elevator-3</a></strong> to <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-next-word-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/g9w95mbo' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/g9w95mbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  94%|█████████▎| 253/270 [00:20<00:02,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.2376, Validation Perplexity: 191.1554, Validation Accuracy: 0.2721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 270/270 [00:21<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 6.3893, Train Perplexity: 1996.8801, Train Accuracy: 0.1848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  86%|████████▌ | 232/270 [00:18<00:06,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.9199, Validation Perplexity: 138.7694, Validation Accuracy: 0.2935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 270/270 [00:21<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 5.1983, Train Perplexity: 182.9008, Train Accuracy: 0.2532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  79%|███████▊  | 212/270 [00:17<00:09,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.8017, Validation Perplexity: 123.4015, Validation Accuracy: 0.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 270/270 [00:21<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 4.6097, Train Perplexity: 100.8756, Train Accuracy: 0.2893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  71%|███████   | 192/270 [00:15<00:12,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.7992, Validation Perplexity: 123.2223, Validation Accuracy: 0.3042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 270/270 [00:22<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 4.1611, Train Perplexity: 64.4575, Train Accuracy: 0.3194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  64%|██████▎   | 172/270 [00:14<00:16,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.8403, Validation Perplexity: 128.6397, Validation Accuracy: 0.3030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 270/270 [00:22<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 3.7786, Train Perplexity: 43.9582, Train Accuracy: 0.3488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  56%|█████▋    | 152/270 [00:12<00:19,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.9257, Validation Perplexity: 140.2911, Validation Accuracy: 0.3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 270/270 [00:22<00:00, 12.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 3.4383, Train Perplexity: 31.2624, Train Accuracy: 0.3805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  49%|████▉     | 132/270 [00:11<00:23,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.0186, Validation Perplexity: 154.2027, Validation Accuracy: 0.2993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 270/270 [00:22<00:00, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 3.1443, Train Perplexity: 23.3008, Train Accuracy: 0.4134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  41%|████▏     | 112/270 [00:09<00:26,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.0936, Validation Perplexity: 166.3232, Validation Accuracy: 0.2967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 270/270 [00:22<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 2.9115, Train Perplexity: 18.4464, Train Accuracy: 0.4438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  34%|███▍      | 92/270 [00:08<00:29,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.1552, Validation Perplexity: 177.0880, Validation Accuracy: 0.2943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 270/270 [00:22<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 2.7507, Train Perplexity: 15.6890, Train Accuracy: 0.4672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  27%|██▋       | 72/270 [00:06<00:33,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.1848, Validation Perplexity: 182.4843, Validation Accuracy: 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 270/270 [00:22<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 2.6628, Train Perplexity: 14.3682, Train Accuracy: 0.4812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aad288546ba45c6ab6e85bf6c78b2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.022 MB uploaded (0.004 MB deduped)\\r'), FloatProgress(value=0.384911…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 18.0%"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▂▂▃▃▃▃▃▄▄▄▄▄▅▄▄▅▅▅▅▆▆▅▅▇▆▆▆▇▇▆▆▇▇█▇████</td></tr><tr><td>train_loss</td><td>█▇▆▆▆▅▆▅▄▄▅▅▄▄▄▄▃▃▃▃▂▂▃▃▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_perplexity</td><td>█▆▄▄▃▂▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆████▇▆▆▅</td></tr><tr><td>val_loss</td><td>█▃▁▁▂▃▅▆▇▇</td></tr><tr><td>val_perplexity</td><td>█▃▁▁▂▃▄▅▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.48116</td></tr><tr><td>train_loss</td><td>2.66282</td></tr><tr><td>train_perplexity</td><td>14.36821</td></tr><tr><td>val_accuracy</td><td>0.29254</td></tr><tr><td>val_loss</td><td>5.18484</td></tr><tr><td>val_perplexity</td><td>182.48425</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-elevator-3</strong> at: <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/g9w95mbo' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-next-word-prediction/runs/g9w95mbo</a><br/> View project at: <a href='https://wandb.ai/srhnylmz14/mamba-next-word-prediction' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-next-word-prediction</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240723_151230-g9w95mbo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = train(model, train_dataloader, val_dataloader, num_epochs, lr, device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_model.state_dict(), \"mamba_lm.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
