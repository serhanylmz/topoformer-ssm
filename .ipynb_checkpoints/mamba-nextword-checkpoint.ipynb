{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = d_model * expand\n",
    "        \n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner)\n",
    "        self.conv = nn.Conv1d(self.d_inner, self.d_inner, d_conv, padding=d_conv-1, groups=self.d_inner)\n",
    "        \n",
    "        self.x_proj = nn.Linear(self.d_inner, self.d_state)\n",
    "        self.dt_proj = nn.Linear(self.d_inner, self.d_state)\n",
    "        \n",
    "        self.A = nn.Parameter(torch.randn(self.d_state))\n",
    "        self.D = nn.Parameter(torch.randn(self.d_state, self.d_inner))\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, L, _ = x.shape\n",
    "        \n",
    "        x = self.in_proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv(x)[:, :, :L]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        x_ssm = self.x_proj(x)\n",
    "        dt = F.softplus(self.dt_proj(x))\n",
    "        \n",
    "        A = -torch.exp(self.A).view(1, 1, -1)\n",
    "        D = self.D\n",
    "        \n",
    "        dA = torch.exp(A * dt)\n",
    "        dB = (1 - dA) / A\n",
    "        \n",
    "        z = torch.zeros(B, self.d_state, device=x.device)\n",
    "        output = []\n",
    "        for i in range(L):\n",
    "            z = dA[:, i] * z + dB[:, i] * x_ssm[:, i]\n",
    "            output.append(z)\n",
    "        z = torch.stack(output, dim=1)\n",
    "        \n",
    "        y = torch.einsum('bls,si->bli', z, D)\n",
    "        \n",
    "        y = self.out_proj(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class MambaLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_state, d_conv, expand, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([MambaBlock(d_model, d_state, d_conv, expand) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = self.lm_head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def load_ptb_data(batch_size):\n",
    "    path = 'ptb.train.txt'\n",
    "    if not os.path.exists(path):\n",
    "        url = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\"\n",
    "        print(f\"Downloading dataset from {url}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Download completed.\")\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = f.read().split()\n",
    "    \n",
    "    vocab = ['<unk>', '<eos>'] + sorted(set(raw_data))\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "    \n",
    "    data = [word_to_idx[word] for word in raw_data]\n",
    "    \n",
    "    train_data = data[:int(0.8*len(data))]\n",
    "    val_data = data[int(0.8*len(data)):int(0.9*len(data))]\n",
    "    test_data = data[int(0.9*len(data)):]\n",
    "    \n",
    "    return train_data, val_data, test_data, len(vocab), idx_to_word\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.data[index:index+self.seq_length]),\n",
    "            torch.tensor(self.data[index+1:index+self.seq_length+1])\n",
    "        )\n",
    "\n",
    "def get_data_loaders(train_data, val_data, test_data, batch_size, seq_length):\n",
    "    train_dataset = TextDataset(train_data, seq_length)\n",
    "    val_dataset = TextDataset(val_data, seq_length)\n",
    "    test_dataset = TextDataset(test_data, seq_length)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device, epoch, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                        desc=f\"Epoch {epoch}\", ncols=100)\n",
    "    \n",
    "    for batch, (data, target) in progress_bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Increased clip value\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / (batch + 1)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{avg_loss:.4f}',\n",
    "            'perplexity': f'{math.exp(avg_loss):.2f}'\n",
    "        })\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": batch,\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"train_perplexity\": math.exp(avg_loss),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output.view(-1, output.size(-1)), target.view(-1)).item() * data.size(0)\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "def generate_text(model, idx_to_word, device, start_text=\"The\", max_length=50):\n",
    "    model.eval()\n",
    "    words = start_text.split()\n",
    "    word_to_idx = {v: k for k, v in idx_to_word.items()}\n",
    "    \n",
    "    input_seq = [word_to_idx.get(w, word_to_idx['<unk>']) for w in words]\n",
    "    input_seq = torch.tensor([input_seq]).to(device)\n",
    "    \n",
    "    generated_words = words.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output = model(input_seq)\n",
    "            next_word_idx = output[:, -1, :].argmax(dim=-1).item()\n",
    "            next_word = idx_to_word[next_word_idx]\n",
    "            generated_words.append(next_word)\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_word_idx]]).to(device)], dim=1)\n",
    "            \n",
    "            if next_word == '<eos>':\n",
    "                break\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "def train_mamba(batch_size=128, seq_length=64, epochs=30, lr=0.0003, d_model=512, d_state=64, d_conv=4, expand=2, num_layers=8):\n",
    "    wandb.init(project=\"mamba-lm\", config={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"d_model\": d_model,\n",
    "        \"d_state\": d_state,\n",
    "        \"d_conv\": d_conv,\n",
    "        \"expand\": expand,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        train_data, val_data, test_data, vocab_size, idx_to_word = load_ptb_data(batch_size)\n",
    "        train_loader, val_loader, test_loader = get_data_loaders(train_data, val_data, test_data, batch_size, seq_length)\n",
    "        print(f\"Data loaded. Vocabulary size: {vocab_size}\")\n",
    "\n",
    "        print(\"Initializing model...\")\n",
    "        model = MambaLM(vocab_size, d_model, d_state, d_conv, expand, num_layers).to(device)\n",
    "        wandb.watch(model, log_freq=100)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        # Warmup and cosine decay scheduler\n",
    "        total_steps = epochs * len(train_loader)\n",
    "        warmup_steps = int(0.1 * total_steps)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = train(model, train_loader, optimizer, criterion, device, epoch, scheduler)\n",
    "            val_loss = evaluate(model, val_loader, criterion, device)\n",
    "            print(f'Epoch {epoch} - Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Perplexity: {math.exp(train_loss):.2f}, '\n",
    "                  f'Val Loss: {val_loss:.4f}, '\n",
    "                  f'Val Perplexity: {math.exp(val_loss):.2f}')\n",
    "            \n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_perplexity\": math.exp(train_loss),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_perplexity\": math.exp(val_loss),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_path = 'mamba_model_best.pt'\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'idx_to_word': idx_to_word\n",
    "                }, save_path)\n",
    "                wandb.save('mamba_model_best.pt')\n",
    "            \n",
    "            sample_text = generate_text(model, idx_to_word, device, start_text=\"The\", max_length=50)\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"generated_text\": wandb.Html(sample_text.replace('\\n', '<br>'))\n",
    "            })\n",
    "\n",
    "        checkpoint = torch.load('mamba_model_best.pt')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        idx_to_word = checkpoint['idx_to_word']\n",
    "        print(\"Model and idx_to_word loaded successfully.\")\n",
    "        \n",
    "        test_loss = evaluate(model, test_loader, criterion, device)\n",
    "        test_perplexity = math.exp(test_loss)\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Perplexity: {test_perplexity:.2f}')\n",
    "        wandb.log({\"test_loss\": test_loss, \"test_perplexity\": test_perplexity})\n",
    "\n",
    "        return model, idx_to_word\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        wandb.finish()\n",
    "        return None, None\n",
    "\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/wandb/__main__.py\", line 3, in <module>\n",
      "    cli.cli(prog_name=\"python -m wandb\")\n",
      "  File \"/cta/users/serhan.yilmaz/.local/lib/python3.10/site-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/cta/users/serhan.yilmaz/.local/lib/python3.10/site-packages/click/core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/cta/users/serhan.yilmaz/.local/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/cta/users/serhan.yilmaz/.local/lib/python3.10/site-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/cta/users/serhan.yilmaz/.local/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/wandb/cli/cli.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/wandb/cli/cli.py\", line 289, in service\n",
      "    server.serve()\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/wandb/sdk/service/server.py\", line 114, in serve\n",
      "    self._inform_used_ports(sock_port=sock_port)\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/wandb/sdk/service/server.py\", line 52, in _inform_used_ports\n",
      "    pf.write(self._port_fname)\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/wandb/sdk/service/port_file.py\", line 21, in write\n",
      "    f = tempfile.NamedTemporaryFile(prefix=bname, dir=dname, mode=\"w\", delete=False)\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/tempfile.py\", line 575, in NamedTemporaryFile\n",
      "    file = _io.open(dir, mode, buffering=buffering,\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/tempfile.py\", line 572, in opener\n",
      "    fd, name = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\n",
      "  File \"/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/tempfile.py\", line 256, in _mkstemp_inner\n",
      "    fd = _os.open(file, flags, 0o600)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp2ytzkgs3/port-387512.txt1xm9ov7q'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrhnylmz14\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cta/users/serhan.yilmaz/llama3-phi3/mamba/topoformer-ssm/wandb/run-20240709_205957-lmru1yzi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srhnylmz14/mamba-lm/runs/lmru1yzi' target=\"_blank\">brisk-glitter-6</a></strong> to <a href='https://wandb.ai/srhnylmz14/mamba-lm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srhnylmz14/mamba-lm' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srhnylmz14/mamba-lm/runs/lmru1yzi' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm/runs/lmru1yzi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Data loaded. Vocabulary size: 10001\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████| 694/694 [02:19<00:00,  4.98it/s, loss=6.6075, perplexity=740.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 6.6075, Train Perplexity: 740.60, Val Loss: 6.4450, Val Perplexity: 629.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████| 694/694 [01:38<00:00,  7.02it/s, loss=6.1851, perplexity=485.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 6.1851, Train Perplexity: 485.46, Val Loss: 6.2656, Val Perplexity: 526.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  12%|██▍                  | 81/694 [00:11<01:26,  7.05it/s, loss=6.0861, perplexity=439.69]"
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.login()\n",
    "    model, idx_to_word = train_mamba(batch_size=1024, seq_length=35, epochs=1000, lr=0.0001, d_model=256, d_state=16, d_conv=4, expand=2, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and idx_to_word loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters (must match the parameters used during training)\n",
    "vocab_size = 10001\n",
    "d_model = 256\n",
    "d_state = 16\n",
    "d_conv = 4\n",
    "expand = 2\n",
    "num_layers = 4\n",
    "\n",
    "# Instantiate the model\n",
    "model = MambaLM(vocab_size, d_model, d_state, d_conv, expand, num_layers)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('mamba_model_best.pt')\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the idx_to_word dictionary\n",
    "idx_to_word = checkpoint['idx_to_word']\n",
    "\n",
    "print(\"Model and idx_to_word loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The quick brown the <unk> <unk> the <unk> <unk> the <unk> <unk> the <unk> <unk> the <unk> <unk> the <unk> <unk> the <unk> <unk> the <unk> <unk> the <unk> <unk> the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "## To be tried after the training is done\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "generated_text = generate_text(model, idx_to_word, device, start_text=\"The quick brown\", max_length=50)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
