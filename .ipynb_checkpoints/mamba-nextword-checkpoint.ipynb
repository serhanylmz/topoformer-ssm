{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = d_model * expand\n",
    "        \n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner)\n",
    "        self.conv = nn.Conv1d(self.d_inner, self.d_inner, d_conv, padding=d_conv-1, groups=self.d_inner)\n",
    "        \n",
    "        self.x_proj = nn.Linear(self.d_inner, self.d_state)\n",
    "        self.dt_proj = nn.Linear(self.d_inner, self.d_state)\n",
    "        \n",
    "        self.A = nn.Parameter(torch.randn(self.d_state))\n",
    "        self.D = nn.Parameter(torch.randn(self.d_state, self.d_inner))\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, L, _ = x.shape\n",
    "        \n",
    "        x = self.in_proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv(x)[:, :, :L]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        x_ssm = self.x_proj(x)\n",
    "        dt = F.softplus(self.dt_proj(x))\n",
    "        \n",
    "        A = -torch.exp(self.A).view(1, 1, -1)\n",
    "        D = self.D\n",
    "        \n",
    "        dA = torch.exp(A * dt)\n",
    "        dB = (1 - dA) / A\n",
    "        \n",
    "        z = torch.zeros(B, self.d_state, device=x.device)\n",
    "        output = []\n",
    "        for i in range(L):\n",
    "            z = dA[:, i] * z + dB[:, i] * x_ssm[:, i]\n",
    "            output.append(z)\n",
    "        z = torch.stack(output, dim=1)\n",
    "        \n",
    "        y = torch.einsum('bls,si->bli', z, D)\n",
    "        \n",
    "        y = self.out_proj(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class MambaLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_state, d_conv, expand, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([MambaBlock(d_model, d_state, d_conv, expand) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = self.lm_head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def load_ptb_data(batch_size):\n",
    "    path = 'ptb.train.txt'\n",
    "    if not os.path.exists(path):\n",
    "        url = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\"\n",
    "        print(f\"Downloading dataset from {url}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Download completed.\")\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = f.read().split()\n",
    "    \n",
    "    vocab = ['<unk>', '<eos>'] + sorted(set(raw_data))\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "    \n",
    "    data = [word_to_idx[word] for word in raw_data]\n",
    "    \n",
    "    train_data = data[:int(0.8*len(data))]\n",
    "    val_data = data[int(0.8*len(data)):int(0.9*len(data))]\n",
    "    test_data = data[int(0.9*len(data)):]\n",
    "    \n",
    "    return train_data, val_data, test_data, len(vocab), idx_to_word\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.data[index:index+self.seq_length]),\n",
    "            torch.tensor(self.data[index+1:index+self.seq_length+1])\n",
    "        )\n",
    "\n",
    "def get_data_loaders(train_data, val_data, test_data, batch_size, seq_length):\n",
    "    train_dataset = TextDataset(train_data, seq_length)\n",
    "    val_dataset = TextDataset(val_data, seq_length)\n",
    "    test_dataset = TextDataset(test_data, seq_length)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device, epoch, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                        desc=f\"Epoch {epoch}\", ncols=100)\n",
    "    \n",
    "    for batch, (data, target) in progress_bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Increased clip value\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / (batch + 1)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{avg_loss:.4f}',\n",
    "            'perplexity': f'{math.exp(avg_loss):.2f}'\n",
    "        })\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": batch,\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"train_perplexity\": math.exp(avg_loss),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output.view(-1, output.size(-1)), target.view(-1)).item() * data.size(0)\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "def generate_text(model, idx_to_word, device, start_text=\"The\", max_length=50):\n",
    "    model.eval()\n",
    "    words = start_text.split()\n",
    "    word_to_idx = {v: k for k, v in idx_to_word.items()}\n",
    "    \n",
    "    input_seq = [word_to_idx.get(w, word_to_idx['<unk>']) for w in words]\n",
    "    input_seq = torch.tensor([input_seq]).to(device)\n",
    "    \n",
    "    generated_words = words.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output = model(input_seq)\n",
    "            next_word_idx = output[:, -1, :].argmax(dim=-1).item()\n",
    "            next_word = idx_to_word[next_word_idx]\n",
    "            generated_words.append(next_word)\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_word_idx]]).to(device)], dim=1)\n",
    "            \n",
    "            if next_word == '<eos>':\n",
    "                break\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "def train_mamba(batch_size=128, seq_length=64, epochs=30, lr=0.0003, d_model=512, d_state=64, d_conv=4, expand=2, num_layers=8):\n",
    "    wandb.init(project=\"mamba-lm\", config={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"seq_length\": seq_length,\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"d_model\": d_model,\n",
    "        \"d_state\": d_state,\n",
    "        \"d_conv\": d_conv,\n",
    "        \"expand\": expand,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        train_data, val_data, test_data, vocab_size, idx_to_word = load_ptb_data(batch_size)\n",
    "        train_loader, val_loader, test_loader = get_data_loaders(train_data, val_data, test_data, batch_size, seq_length)\n",
    "        print(f\"Data loaded. Vocabulary size: {vocab_size}\")\n",
    "\n",
    "        print(\"Initializing model...\")\n",
    "        model = MambaLM(vocab_size, d_model, d_state, d_conv, expand, num_layers).to(device)\n",
    "        wandb.watch(model, log_freq=100)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        # Warmup and cosine decay scheduler\n",
    "        total_steps = epochs * len(train_loader)\n",
    "        warmup_steps = int(0.1 * total_steps)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = train(model, train_loader, optimizer, criterion, device, epoch, scheduler)\n",
    "            val_loss = evaluate(model, val_loader, criterion, device)\n",
    "            print(f'Epoch {epoch} - Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Perplexity: {math.exp(train_loss):.2f}, '\n",
    "                  f'Val Loss: {val_loss:.4f}, '\n",
    "                  f'Val Perplexity: {math.exp(val_loss):.2f}')\n",
    "            \n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_perplexity\": math.exp(train_loss),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_perplexity\": math.exp(val_loss),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_path = 'mamba_model_best.pt'\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'idx_to_word': idx_to_word\n",
    "                }, save_path)\n",
    "                wandb.save('mamba_model_best.pt')\n",
    "            \n",
    "            sample_text = generate_text(model, idx_to_word, device, start_text=\"The\", max_length=50)\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"generated_text\": wandb.Html(sample_text.replace('\\n', '<br>'))\n",
    "            })\n",
    "\n",
    "        checkpoint = torch.load('mamba_model_best.pt')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        idx_to_word = checkpoint['idx_to_word']\n",
    "        print(\"Model and idx_to_word loaded successfully.\")\n",
    "        \n",
    "        test_loss = evaluate(model, test_loader, criterion, device)\n",
    "        test_perplexity = math.exp(test_loss)\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Perplexity: {test_perplexity:.2f}')\n",
    "        wandb.log({\"test_loss\": test_loss, \"test_perplexity\": test_perplexity})\n",
    "\n",
    "        return model, idx_to_word\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        wandb.finish()\n",
    "        return None, None\n",
    "\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrhnylmz14\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cta/users/serhan.yilmaz/llama3-phi3/mamba/topoformer-ssm/wandb/run-20240710_184214-hmxjzrr1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srhnylmz14/mamba-lm/runs/hmxjzrr1' target=\"_blank\">efficient-salad-11</a></strong> to <a href='https://wandb.ai/srhnylmz14/mamba-lm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srhnylmz14/mamba-lm' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srhnylmz14/mamba-lm/runs/hmxjzrr1' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm/runs/hmxjzrr1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Data loaded. Vocabulary size: 10001\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  16%|██▊               | 442/2774 [06:04<32:01,  1.21it/s, loss=7.2046, perplexity=1345.67]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 1.2%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▃▅▆█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▃▄▆█</td></tr><tr><td>train_loss</td><td>█▅▃▂▁</td></tr><tr><td>train_perplexity</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>400</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>train_loss</td><td>7.2604</td></tr><tr><td>train_perplexity</td><td>1422.82352</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-salad-11</strong> at: <a href='https://wandb.ai/srhnylmz14/mamba-lm/runs/hmxjzrr1' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm/runs/hmxjzrr1</a><br/> View project at: <a href='https://wandb.ai/srhnylmz14/mamba-lm' target=\"_blank\">https://wandb.ai/srhnylmz14/mamba-lm</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240710_184214-hmxjzrr1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m----> 4\u001b[0m     model, idx_to_word \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mamba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_conv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 241\u001b[0m, in \u001b[0;36mtrain_mamba\u001b[0;34m(batch_size, seq_length, epochs, lr, d_model, d_state, d_conv, expand, num_layers)\u001b[0m\n\u001b[1;32m    238\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 241\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    244\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mexp(train_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    245\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    246\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mexp(val_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 148\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, device, epoch, scheduler)\u001b[0m\n\u001b[1;32m    146\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m    147\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 148\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Increased clip value\u001b[39;00m\n\u001b[1;32m    150\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.login()\n",
    "    model, idx_to_word = train_mamba(batch_size=512, seq_length=64, epochs=50, lr=0.001, d_model=512, d_state=32, d_conv=4, expand=2, num_layers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_text(model, idx_to_word, device, start_text=\"The quick brown\", max_length=50)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters (must match the parameters used during training)\n",
    "vocab_size = 10001\n",
    "d_model = 512\n",
    "d_state = 32\n",
    "d_conv = 4\n",
    "expand = 2\n",
    "num_layers = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = MambaLM(vocab_size, d_model, d_state, d_conv, expand, num_layers)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('mamba_model_best.pt')\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the idx_to_word dictionary\n",
    "idx_to_word = checkpoint['idx_to_word']\n",
    "\n",
    "print(\"Model and idx_to_word loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To be tried after the training is done\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "generated_text = generate_text(model, idx_to_word, device, start_text=\"She forgot her books at\", max_length=50)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
